\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Week 1}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Train/Dev/Test Sets}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Bias/Variance}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Basic Recipe for Machine Learning}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Regularization}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Why Regularization Reduces Overfitting}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Dropout Regularization}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Understanding Dropout}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Other Regularization Methods}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Normalizing Inputs}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Exploding/Vanishing Gradients}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11}Weight Initialization in a Deep Network}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12}Numerical Approximation of Gradients}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13}Gradient Checking}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Week 2}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Mini-Batch Gradient Descent}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Understanding Mini-Batch Gradient Descent}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Exponentially Weighted Averages}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Bias Correction of Exponentially Weighted Averages}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Gradient Descent with Momentum}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}RMSProp}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Adam Optimization Algorithm}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Learning Rate Decay}{19}\protected@file@percent }
