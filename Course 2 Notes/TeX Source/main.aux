\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Week 1}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Train/Dev/Test Sets}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Bias/Variance}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Basic Recipe for Machine Learning}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Regularization}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Why Regularization Reduces Overfitting}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Dropout Regularization}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Understanding Dropout}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Other Regularization Methods}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Normalizing Inputs}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Exploding/Vanishing Gradients}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11}Weight Initialization in a Deep Network}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12}Numerical Approximation of Gradients}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13}Gradient Checking}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Week 2}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Mini-Batch Gradient Descent}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Understanding Mini-Batch Gradient Descent}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Exponentially Weighted Averages}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Bias Correction of Exponentially Weighted Averages}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Gradient Descent with Momentum}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}RMSProp}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Adam Optimization Algorithm}{18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Learning Rate Decay}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Week 3}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Tuning Process}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Using an Appropriate Scale}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Hyperparameter Tuning in Practice}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Normalizing Activations in a Network}{22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Fitting Batch Norm into Neural Networks}{23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Why Does Batch Norm Work?}{23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Batch Norm at Test Time}{24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Softmax Regression}{25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9}Training a Softmax Classifier}{25}\protected@file@percent }
