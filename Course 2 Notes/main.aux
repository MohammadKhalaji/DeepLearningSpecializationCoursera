\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Week 1}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Train/Dev/Test Sets}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Bias/Variance}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Basic Recipe for Machine Learning}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Regularization}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Why Regularization Reduces Overfitting}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Dropout Regularization}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Understanding Dropout}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Other Regularization Methods}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Normalizing Inputs}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Exploding/Vanishing Gradients}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11}Weight Initialization in a Deep Network}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12}Numerical Approximation of Gradients}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13}Gradient Checking}{12}\protected@file@percent }
